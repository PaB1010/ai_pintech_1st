{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dde972c-8d95-4e74-97ea-b4d7f9c8749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 완성 (문서내 문장 가공 후 다음 문장 예측)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c54461-813d-41d6-a257-9df1eee60d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\mldl\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\mldl\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\mldl\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mldl\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\mldl\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80c133e-f903-4d52-8966-d2cc0afefe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt')\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0deaa3-0558-4ed1-a496-b64aea1f7237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                       Language is never, ever, ever, random\\n\\n                                                               ADAM KILGARRIFF\\n\\n\\n\\n\\nAbstract\\nLanguage users never choose words randomly, and language is essentially\\nnon-random. Statistical hypothesis testing uses a null hypothesis, which\\nposits randomness. Hence, when we look at linguistic phenomena in cor-\\npora, the null hypothesis will never be true. Moreover, where there is enough\\ndata, we shall (almost) always be able to establish '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe98ea7-0fcf-480f-95c5-c6f95d45e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize & sent_tokenize\n",
    "# 활용해 대문자 -> 소문자 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dda4ae0-91de-4269-b203-366085323334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabecdf2-4dd1-4d0c-a9f4-8fe5f1a1c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장별 Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3817af2b-e645-4c79-bfaa-7d30f08581d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                       Language is never, ever, ever, random\\n\\n                                                               ADAM KILGARRIFF\\n\\n\\n\\n\\nAbstract\\nLanguage users never choose words randomly, and language is essentially\\nnon-random.',\n",
       " 'Statistical hypothesis testing uses a null hypothesis, which\\nposits randomness.',\n",
       " 'Hence, when we look at linguistic phenomena in cor-\\npora, the null hypothesis will never be true.',\n",
       " 'Moreover, where there is enough\\ndata, we shall (almost) always be able to establish that it is not true.',\n",
       " 'In\\ncorpus studies, we frequently do have enough data, so the fact that a rela-\\ntion between two phenomena is demonstrably non-random, does not sup-\\nport the inference that it is not arbitrary.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18c615-d03b-4eae-bca4-16f95a9857dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 Token\n",
    "# 1. List 컴프리헨션(일괄 Word Token화)\n",
    "# 2. map 관련 편의 메서드(str.lower) 이용 대문자-> 소문자\n",
    "# 3. 다시 list 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8fe94b1-c20c-46e1-a4bc-766471bce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = [list(map(str.lower, word_tokenize(sentence))) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "946d51ff-8432-4c22-868b-afd9684574c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['language',\n",
       "  'is',\n",
       "  'never',\n",
       "  ',',\n",
       "  'ever',\n",
       "  ',',\n",
       "  'ever',\n",
       "  ',',\n",
       "  'random',\n",
       "  'adam',\n",
       "  'kilgarriff',\n",
       "  'abstract',\n",
       "  'language',\n",
       "  'users',\n",
       "  'never',\n",
       "  'choose',\n",
       "  'words',\n",
       "  'randomly',\n",
       "  ',',\n",
       "  'and',\n",
       "  'language',\n",
       "  'is',\n",
       "  'essentially',\n",
       "  'non-random',\n",
       "  '.'],\n",
       " ['statistical',\n",
       "  'hypothesis',\n",
       "  'testing',\n",
       "  'uses',\n",
       "  'a',\n",
       "  'null',\n",
       "  'hypothesis',\n",
       "  ',',\n",
       "  'which',\n",
       "  'posits',\n",
       "  'randomness',\n",
       "  '.'],\n",
       " ['hence',\n",
       "  ',',\n",
       "  'when',\n",
       "  'we',\n",
       "  'look',\n",
       "  'at',\n",
       "  'linguistic',\n",
       "  'phenomena',\n",
       "  'in',\n",
       "  'cor-',\n",
       "  'pora',\n",
       "  ',',\n",
       "  'the',\n",
       "  'null',\n",
       "  'hypothesis',\n",
       "  'will',\n",
       "  'never',\n",
       "  'be',\n",
       "  'true',\n",
       "  '.'],\n",
       " ['moreover',\n",
       "  ',',\n",
       "  'where',\n",
       "  'there',\n",
       "  'is',\n",
       "  'enough',\n",
       "  'data',\n",
       "  ',',\n",
       "  'we',\n",
       "  'shall',\n",
       "  '(',\n",
       "  'almost',\n",
       "  ')',\n",
       "  'always',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'establish',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'true',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'corpus',\n",
       "  'studies',\n",
       "  ',',\n",
       "  'we',\n",
       "  'frequently',\n",
       "  'do',\n",
       "  'have',\n",
       "  'enough',\n",
       "  'data',\n",
       "  ',',\n",
       "  'so',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'a',\n",
       "  'rela-',\n",
       "  'tion',\n",
       "  'between',\n",
       "  'two',\n",
       "  'phenomena',\n",
       "  'is',\n",
       "  'demonstrably',\n",
       "  'non-random',\n",
       "  ',',\n",
       "  'does',\n",
       "  'not',\n",
       "  'sup-',\n",
       "  'port',\n",
       "  'the',\n",
       "  'inference',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'arbitrary',\n",
       "  '.']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea07c7-3c32-4f37-95ea-fd34f5d9811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EveryGram + Flatten 학습 Data 가공 편의 Lib\n",
    "# 문장 시작-끝 Token 생성 및 1차원 배열화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4b2efe5-e5e9-4c17-a4d4-c6a0f399e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cdd52aa-68ca-4caf-be51-1e16aea47de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data -> everygram(max_len = 3)\n",
    "train_data, padded_sentences = padded_everygram_pipeline(3, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "200a7cf1-06cd-47a4-a890-d1266e507fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', '<s>'),\n",
       " ('<s>', '<s>', 'language'),\n",
       " ('<s>',),\n",
       " ('<s>', 'language'),\n",
       " ('<s>', 'language', 'is'),\n",
       " ('language',),\n",
       " ('language', 'is'),\n",
       " ('language', 'is', 'never'),\n",
       " ('is',),\n",
       " ('is', 'never'),\n",
       " ('is', 'never', ','),\n",
       " ('never',),\n",
       " ('never', ','),\n",
       " ('never', ',', 'ever'),\n",
       " (',',),\n",
       " (',', 'ever'),\n",
       " (',', 'ever', ','),\n",
       " ('ever',),\n",
       " ('ever', ','),\n",
       " ('ever', ',', 'ever'),\n",
       " (',',),\n",
       " (',', 'ever'),\n",
       " (',', 'ever', ','),\n",
       " ('ever',),\n",
       " ('ever', ','),\n",
       " ('ever', ',', 'random'),\n",
       " (',',),\n",
       " (',', 'random'),\n",
       " (',', 'random', 'adam'),\n",
       " ('random',),\n",
       " ('random', 'adam'),\n",
       " ('random', 'adam', 'kilgarriff'),\n",
       " ('adam',),\n",
       " ('adam', 'kilgarriff'),\n",
       " ('adam', 'kilgarriff', 'abstract'),\n",
       " ('kilgarriff',),\n",
       " ('kilgarriff', 'abstract'),\n",
       " ('kilgarriff', 'abstract', 'language'),\n",
       " ('abstract',),\n",
       " ('abstract', 'language'),\n",
       " ('abstract', 'language', 'users'),\n",
       " ('language',),\n",
       " ('language', 'users'),\n",
       " ('language', 'users', 'never'),\n",
       " ('users',),\n",
       " ('users', 'never'),\n",
       " ('users', 'never', 'choose'),\n",
       " ('never',),\n",
       " ('never', 'choose'),\n",
       " ('never', 'choose', 'words'),\n",
       " ('choose',),\n",
       " ('choose', 'words'),\n",
       " ('choose', 'words', 'randomly'),\n",
       " ('words',),\n",
       " ('words', 'randomly'),\n",
       " ('words', 'randomly', ','),\n",
       " ('randomly',),\n",
       " ('randomly', ','),\n",
       " ('randomly', ',', 'and'),\n",
       " (',',),\n",
       " (',', 'and'),\n",
       " (',', 'and', 'language'),\n",
       " ('and',),\n",
       " ('and', 'language'),\n",
       " ('and', 'language', 'is'),\n",
       " ('language',),\n",
       " ('language', 'is'),\n",
       " ('language', 'is', 'essentially'),\n",
       " ('is',),\n",
       " ('is', 'essentially'),\n",
       " ('is', 'essentially', 'non-random'),\n",
       " ('essentially',),\n",
       " ('essentially', 'non-random'),\n",
       " ('essentially', 'non-random', '.'),\n",
       " ('non-random',),\n",
       " ('non-random', '.'),\n",
       " ('non-random', '.', '</s>'),\n",
       " ('.',),\n",
       " ('.', '</s>'),\n",
       " ('.', '</s>', '</s>'),\n",
       " ('</s>',),\n",
       " ('</s>', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(train_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a06de93a-45b1-4084-96be-8b8a63ddb109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " 'language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'random',\n",
       " 'adam',\n",
       " 'kilgarriff',\n",
       " 'abstract',\n",
       " 'language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " ',',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'statistical',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'uses',\n",
       " 'a',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " ',',\n",
       " 'which',\n",
       " 'posits',\n",
       " 'randomness',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'hence',\n",
       " ',',\n",
       " 'when',\n",
       " 'we',\n",
       " 'look',\n",
       " 'at',\n",
       " 'linguistic',\n",
       " 'phenomena',\n",
       " 'in',\n",
       " 'cor-',\n",
       " 'pora',\n",
       " ',',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'will',\n",
       " 'never',\n",
       " 'be',\n",
       " 'true',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'moreover',\n",
       " ',',\n",
       " 'where',\n",
       " 'there',\n",
       " 'is',\n",
       " 'enough',\n",
       " 'data',\n",
       " ',',\n",
       " 'we',\n",
       " 'shall',\n",
       " '(',\n",
       " 'almost',\n",
       " ')',\n",
       " 'always',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'establish',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'true',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'in',\n",
       " 'corpus',\n",
       " 'studies',\n",
       " ',',\n",
       " 'we',\n",
       " 'frequently',\n",
       " 'do',\n",
       " 'have',\n",
       " 'enough',\n",
       " 'data',\n",
       " ',',\n",
       " 'so',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'a',\n",
       " 'rela-',\n",
       " 'tion',\n",
       " 'between',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " 'is',\n",
       " 'demonstrably',\n",
       " 'non-random',\n",
       " ',',\n",
       " 'does',\n",
       " 'not',\n",
       " 'sup-',\n",
       " 'port',\n",
       " 'the',\n",
       " 'inference',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'arbitrary',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'present',\n",
       " 'experimental',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'how',\n",
       " 'arbitrary',\n",
       " 'associations',\n",
       " 'between',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'and',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'systematically',\n",
       " 'non-random',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'review',\n",
       " 'literature',\n",
       " 'in',\n",
       " 'which',\n",
       " 'hypothesis',\n",
       " 'test-',\n",
       " 'ing',\n",
       " 'has',\n",
       " 'been',\n",
       " 'used',\n",
       " ',',\n",
       " 'and',\n",
       " 'show',\n",
       " 'how',\n",
       " 'it',\n",
       " 'has',\n",
       " 'often',\n",
       " 'led',\n",
       " 'to',\n",
       " 'unhelpful',\n",
       " 'or',\n",
       " 'mislead-',\n",
       " 'ing',\n",
       " 'results',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'keywords',\n",
       " ':',\n",
       " '쎲쎲쎲',\n",
       " '1',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'introduction',\n",
       " 'any',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " 'might',\n",
       " 'or',\n",
       " 'might',\n",
       " 'not',\n",
       " 'be',\n",
       " 'related',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " 'pos-',\n",
       " 'sibilities',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'association',\n",
       " 'is',\n",
       " 'random',\n",
       " ',',\n",
       " 'arbitrary',\n",
       " ',',\n",
       " 'motivated',\n",
       " 'or',\n",
       " 'pre-',\n",
       " 'dictable',\n",
       " '(',\n",
       " 'r',\n",
       " ',',\n",
       " 'a',\n",
       " ',',\n",
       " 'm',\n",
       " ',',\n",
       " 'p',\n",
       " ')',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'bulk',\n",
       " 'of',\n",
       " 'linguistic',\n",
       " 'questions',\n",
       " 'concern',\n",
       " 'the',\n",
       " 'dis-',\n",
       " 'tinction',\n",
       " 'between',\n",
       " 'a',\n",
       " 'and',\n",
       " 'm.',\n",
       " 'a',\n",
       " 'linguistic',\n",
       " 'account',\n",
       " 'of',\n",
       " 'a',\n",
       " 'phenomenon',\n",
       " 'gen-',\n",
       " 'erally',\n",
       " 'gives',\n",
       " 'us',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'view',\n",
       " 'the',\n",
       " 'relation',\n",
       " 'between',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'a',\n",
       " 'verb',\n",
       " '’',\n",
       " 's',\n",
       " 'syntax',\n",
       " 'and',\n",
       " 'its',\n",
       " 'semantics',\n",
       " ',',\n",
       " 'as',\n",
       " 'motivated',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'arbitrary',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'however',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'in',\n",
       " 'general',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'model',\n",
       " 'the',\n",
       " 'a-m',\n",
       " 'distinction',\n",
       " 'mathematically',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'distinction',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'modeled',\n",
       " 'mathematically',\n",
       " 'is',\n",
       " 'between',\n",
       " 'r',\n",
       " 'and',\n",
       " 'not-r',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " ',',\n",
       " 'between',\n",
       " 'random',\n",
       " ',',\n",
       " 'or',\n",
       " 'uncorrelated',\n",
       " ',',\n",
       " 'pairs',\n",
       " 'and',\n",
       " 'pairs',\n",
       " 'where',\n",
       " 'there',\n",
       " 'is',\n",
       " 'some',\n",
       " 'correlation',\n",
       " ',',\n",
       " 'be',\n",
       " 'it',\n",
       " 'arbitrary',\n",
       " ',',\n",
       " 'motivated',\n",
       " 'or',\n",
       " 'predictable.1',\n",
       " 'the',\n",
       " 'mechanism',\n",
       " 'here',\n",
       " 'is',\n",
       " 'hypothesis-testing',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'a',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " ',',\n",
       " 'h0',\n",
       " 'is',\n",
       " 'con-',\n",
       " 'structed',\n",
       " 'to',\n",
       " 'model',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'in',\n",
       " 'which',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'and',\n",
       " 'linguistic',\n",
       " 'theory',\n",
       " '1⫺2',\n",
       " '(',\n",
       " '2005',\n",
       " ')',\n",
       " ',',\n",
       " '263⫺275',\n",
       " '1613-7027/05/0001⫺0263',\n",
       " '쑕',\n",
       " 'walter',\n",
       " 'de',\n",
       " 'gruyter',\n",
       " '264',\n",
       " 'a.',\n",
       " 'kilgarriff',\n",
       " 'the',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mathematics',\n",
       " 'of',\n",
       " 'the',\n",
       " 'random',\n",
       " 'is',\n",
       " 'well',\n",
       " 'under-',\n",
       " 'stood',\n",
       " ',',\n",
       " 'we',\n",
       " 'can',\n",
       " 'compute',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'of',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'given',\n",
       " 'the',\n",
       " 'data',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'if',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'is',\n",
       " 'low',\n",
       " ',',\n",
       " 'we',\n",
       " 'reject',\n",
       " 'h0',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'for',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'is',\n",
       " 'that',\n",
       " 'language',\n",
       " 'is',\n",
       " 'not',\n",
       " 'random',\n",
       " ',',\n",
       " 'so',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'is',\n",
       " 'never',\n",
       " 'true',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'language',\n",
       " 'is',\n",
       " 'not',\n",
       " 'random',\n",
       " 'because',\n",
       " 'we',\n",
       " 'speak',\n",
       " 'or',\n",
       " 'write',\n",
       " 'with',\n",
       " 'purposes',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not',\n",
       " ',',\n",
       " 'indeed',\n",
       " ',',\n",
       " 'without',\n",
       " 'computational',\n",
       " 'help',\n",
       " 'are',\n",
       " 'not',\n",
       " 'capable',\n",
       " 'of',\n",
       " ',',\n",
       " 'producing',\n",
       " 'words',\n",
       " 'or',\n",
       " 'sounds',\n",
       " 'or',\n",
       " 'sentences',\n",
       " 'or',\n",
       " 'documents',\n",
       " 'randomly',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not',\n",
       " 'always',\n",
       " 'have',\n",
       " 'enough',\n",
       " 'data',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " ',',\n",
       " 'but',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'distinct',\n",
       " 'issue',\n",
       " ':',\n",
       " 'wherever',\n",
       " 'there',\n",
       " 'is',\n",
       " 'enough',\n",
       " 'data',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'rejected',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'using',\n",
       " 'language',\n",
       " 'corpora',\n",
       " ',',\n",
       " 'we',\n",
       " 'are',\n",
       " 'frequently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fortunate',\n",
       " 'position',\n",
       " 'of',\n",
       " 'having',\n",
       " 'very',\n",
       " 'large',\n",
       " 'quantities',\n",
       " 'of',\n",
       " 'data',\n",
       " 'at',\n",
       " 'our',\n",
       " 'disposal',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'then',\n",
       " ',',\n",
       " 'even',\n",
       " 'where',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'set',\n",
       " 'up',\n",
       " 'to',\n",
       " 'be',\n",
       " 'linguistically',\n",
       " 'identical',\n",
       " ',',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'is',\n",
       " 'resoundingly',\n",
       " 'defeated',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'in',\n",
       " 'section',\n",
       " '4',\n",
       " ',',\n",
       " 'we',\n",
       " 'present',\n",
       " 'an',\n",
       " 'experiment',\n",
       " 'demonstrating',\n",
       " 'this',\n",
       " 'counterintuitive',\n",
       " 'effect',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'papers',\n",
       " 'in',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'literature',\n",
       " 'where',\n",
       " 'researchers',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'an',\n",
       " 'association',\n",
       " 'was',\n",
       " 'lin-',\n",
       " 'guistically',\n",
       " 'salient',\n",
       " ',',\n",
       " 'or',\n",
       " 'used',\n",
       " 'the',\n",
       " 'confidence',\n",
       " 'with',\n",
       " 'which',\n",
       " 'h0',\n",
       " 'could',\n",
       " 'be',\n",
       " 're-',\n",
       " 'jected',\n",
       " 'as',\n",
       " 'a',\n",
       " 'measure',\n",
       " 'of',\n",
       " 'salience',\n",
       " ',',\n",
       " 'whereas',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'they',\n",
       " 'were',\n",
       " 'merely',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'they',\n",
       " 'had',\n",
       " 'enough',\n",
       " 'data',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'h0',\n",
       " 'with',\n",
       " 'confidence',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'some',\n",
       " 'such',\n",
       " 'cases',\n",
       " 'are',\n",
       " 'reviewed',\n",
       " 'in',\n",
       " 'section',\n",
       " '5',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'has',\n",
       " 'been',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'acquisition',\n",
       " 'of',\n",
       " 'subcategorization',\n",
       " 'frames',\n",
       " 'from',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'this',\n",
       " 'literature',\n",
       " 'is',\n",
       " 'considered',\n",
       " 'in',\n",
       " 'some',\n",
       " 'detail',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'alternatives',\n",
       " 'to',\n",
       " 'inappropriate',\n",
       " 'hy-',\n",
       " 'pothesis-testing',\n",
       " 'are',\n",
       " 'presented',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'before',\n",
       " 'proceeding',\n",
       " ',',\n",
       " 'may',\n",
       " 'i',\n",
       " 'clarify',\n",
       " 'that',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'is',\n",
       " 'in',\n",
       " 'no',\n",
       " 'way',\n",
       " 'critical',\n",
       " 'of',\n",
       " 'using',\n",
       " 'probability',\n",
       " 'models',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'which',\n",
       " 'are',\n",
       " 'based',\n",
       " 'on',\n",
       " 'assumptions',\n",
       " 'of',\n",
       " 'randomness',\n",
       " ',',\n",
       " 'in',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'in',\n",
       " 'general',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'probability',\n",
       " 'models',\n",
       " 'have',\n",
       " 'been',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'a',\n",
       " 'large',\n",
       " 'share',\n",
       " 'of',\n",
       " 'progress',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'decade',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'randomness',\n",
       " 'assumptions',\n",
       " 'are',\n",
       " 'always',\n",
       " 'untrue',\n",
       " ',',\n",
       " 'but',\n",
       " 'that',\n",
       " 'does',\n",
       " 'not',\n",
       " 'preclude',\n",
       " 'them',\n",
       " 'from',\n",
       " 'frequently',\n",
       " 'being',\n",
       " 'useful',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'making',\n",
       " 'false',\n",
       " 'assumptions',\n",
       " 'is',\n",
       " 'often',\n",
       " 'an',\n",
       " 'ingenious',\n",
       " 'way',\n",
       " 'to',\n",
       " 'proceed',\n",
       " ';',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'arises',\n",
       " 'where',\n",
       " 'the',\n",
       " 'literal',\n",
       " 'falsity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'assumption',\n",
       " 'is',\n",
       " 'overlooked',\n",
       " ',',\n",
       " 'and',\n",
       " 'inappropri-',\n",
       " 'ate',\n",
       " 'inferences',\n",
       " 'are',\n",
       " 'drawn',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '2',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'arbitrary',\n",
       " 'and',\n",
       " 'the',\n",
       " 'random',\n",
       " 'in',\n",
       " 'common',\n",
       " 'parlance',\n",
       " ',',\n",
       " 'random',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'are',\n",
       " 'synonyms',\n",
       " ',',\n",
       " 'with',\n",
       " 'diction-',\n",
       " 'aries',\n",
       " 'giving',\n",
       " 'near-identical',\n",
       " 'definitions',\n",
       " ':',\n",
       " 'ldoce',\n",
       " '(',\n",
       " '1995',\n",
       " ')',\n",
       " 'defines',\n",
       " 'random',\n",
       " 'as',\n",
       " 'happening',\n",
       " 'or',\n",
       " 'chosen',\n",
       " 'without',\n",
       " 'any',\n",
       " 'definite',\n",
       " 'plan',\n",
       " ',',\n",
       " 'or',\n",
       " 'pattern',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'as',\n",
       " '1',\n",
       " 'decided',\n",
       " 'or',\n",
       " 'arranged',\n",
       " 'without',\n",
       " 'any',\n",
       " 'reason',\n",
       " 'or',\n",
       " 'plan',\n",
       " ',',\n",
       " 'often',\n",
       " 'unfairly',\n",
       " '…',\n",
       " '2',\n",
       " 'happening',\n",
       " 'or',\n",
       " 'decided',\n",
       " 'by',\n",
       " 'chance',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'a',\n",
       " 'plan',\n",
       " 'language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'random',\n",
       " '265',\n",
       " 'superficially',\n",
       " ',',\n",
       " 'randomness',\n",
       " ',',\n",
       " 'as',\n",
       " 'defined',\n",
       " 'here',\n",
       " ',',\n",
       " 'is',\n",
       " 'what',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'random',\n",
       " 'captures',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'explicit',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'sense',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'statistical',\n",
       " 'independence',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'first',\n",
       " ',',\n",
       " 'we',\n",
       " 'formalize',\n",
       " 'the',\n",
       " 'framework',\n",
       " ':',\n",
       " 'for',\n",
       " 'a',\n",
       " 'population',\n",
       " 'of',\n",
       " 'events',\n",
       " ',',\n",
       " 'the',\n",
       " 'first',\n",
       " 'phenomenon',\n",
       " 'holds',\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66efdcf-0c6f-4769-9685-3efd6c3bbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE (Maximum Likelihood Estimation)\n",
    "# 특정 Token을 뽑았을때 가장 확률이 높은 것을 추정하는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fd6a6c4-c935-43d6-b5cd-e4801f98a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6093a1ce-159e-4c7f-bc93-2d5eae2dfeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-grams\n",
    "model = MLE(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "373481cd-c3fa-4e34-af13-6b3d6213bd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " 'random',\n",
       " 'adam',\n",
       " 'kilgarriff',\n",
       " 'abstract',\n",
       " 'users',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " 'and',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.',\n",
       " '</s>',\n",
       " 'statistical',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'uses',\n",
       " 'a',\n",
       " 'null',\n",
       " 'which',\n",
       " 'posits',\n",
       " 'randomness',\n",
       " 'hence',\n",
       " 'when',\n",
       " 'we',\n",
       " 'look',\n",
       " 'at',\n",
       " 'linguistic',\n",
       " 'phenomena',\n",
       " 'in',\n",
       " 'cor-',\n",
       " 'pora',\n",
       " 'the',\n",
       " 'will',\n",
       " 'be',\n",
       " 'true',\n",
       " 'moreover',\n",
       " 'where',\n",
       " 'there',\n",
       " 'enough',\n",
       " 'data',\n",
       " 'shall',\n",
       " '(',\n",
       " 'almost',\n",
       " ')',\n",
       " 'always',\n",
       " 'able',\n",
       " 'to',\n",
       " 'establish',\n",
       " 'that',\n",
       " 'it',\n",
       " 'not',\n",
       " 'corpus',\n",
       " 'studies',\n",
       " 'frequently',\n",
       " 'do',\n",
       " 'have',\n",
       " 'so',\n",
       " 'fact',\n",
       " 'rela-',\n",
       " 'tion',\n",
       " 'between',\n",
       " 'two',\n",
       " 'demonstrably',\n",
       " 'does',\n",
       " 'sup-',\n",
       " 'port',\n",
       " 'inference',\n",
       " 'arbitrary',\n",
       " 'present',\n",
       " 'experimental',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'how',\n",
       " 'associations',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'systematically',\n",
       " 'review',\n",
       " 'literature',\n",
       " 'test-',\n",
       " 'ing',\n",
       " 'has',\n",
       " 'been',\n",
       " 'used',\n",
       " 'show',\n",
       " 'often',\n",
       " 'led',\n",
       " 'unhelpful',\n",
       " 'or',\n",
       " 'mislead-',\n",
       " 'results',\n",
       " 'keywords',\n",
       " ':',\n",
       " '쎲쎲쎲',\n",
       " '1',\n",
       " 'introduction',\n",
       " 'any',\n",
       " 'might',\n",
       " 'related',\n",
       " 'range',\n",
       " 'pos-',\n",
       " 'sibilities',\n",
       " 'association',\n",
       " 'motivated',\n",
       " 'pre-',\n",
       " 'dictable',\n",
       " 'r',\n",
       " 'm',\n",
       " 'p',\n",
       " 'bulk',\n",
       " 'questions',\n",
       " 'concern',\n",
       " 'dis-',\n",
       " 'tinction',\n",
       " 'm.',\n",
       " 'account',\n",
       " 'phenomenon',\n",
       " 'gen-',\n",
       " 'erally',\n",
       " 'gives',\n",
       " 'us',\n",
       " 'reason',\n",
       " 'view',\n",
       " 'relation',\n",
       " 'for',\n",
       " 'example',\n",
       " 'verb',\n",
       " '’',\n",
       " 's',\n",
       " 'syntax',\n",
       " 'its',\n",
       " 'semantics',\n",
       " 'as',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'however',\n",
       " 'general',\n",
       " 'possible',\n",
       " 'model',\n",
       " 'a-m',\n",
       " 'distinction',\n",
       " 'mathematically',\n",
       " 'can',\n",
       " 'modeled',\n",
       " 'not-r',\n",
       " 'uncorrelated',\n",
       " 'pairs',\n",
       " 'some',\n",
       " 'correlation',\n",
       " 'predictable.1',\n",
       " 'mechanism',\n",
       " 'here',\n",
       " 'hypothesis-testing',\n",
       " 'h0',\n",
       " 'con-',\n",
       " 'structed',\n",
       " 'situation',\n",
       " 'no',\n",
       " 'linguistics',\n",
       " 'theory',\n",
       " '1⫺2',\n",
       " '2005',\n",
       " '263⫺275',\n",
       " '1613-7027/05/0001⫺0263',\n",
       " '쑕',\n",
       " 'walter',\n",
       " 'de',\n",
       " 'gruyter',\n",
       " '264',\n",
       " 'a.',\n",
       " 'mathematics',\n",
       " 'well',\n",
       " 'under-',\n",
       " 'stood',\n",
       " 'compute',\n",
       " 'likelihood',\n",
       " 'given',\n",
       " 'if',\n",
       " 'low',\n",
       " 'reject',\n",
       " 'problem',\n",
       " 'empirical',\n",
       " 'because',\n",
       " 'speak',\n",
       " 'write',\n",
       " 'with',\n",
       " 'purposes',\n",
       " 'indeed',\n",
       " 'without',\n",
       " 'computational',\n",
       " 'help',\n",
       " 'capable',\n",
       " 'producing',\n",
       " 'sounds',\n",
       " 'sentences',\n",
       " 'documents',\n",
       " 'but',\n",
       " 'distinct',\n",
       " 'issue',\n",
       " 'wherever',\n",
       " 'rejected',\n",
       " 'using',\n",
       " 'fortunate',\n",
       " 'position',\n",
       " 'having',\n",
       " 'very',\n",
       " 'large',\n",
       " 'quantities',\n",
       " 'our',\n",
       " 'disposal',\n",
       " 'then',\n",
       " 'even',\n",
       " 'set',\n",
       " 'up',\n",
       " 'linguistically',\n",
       " 'identical',\n",
       " 'resoundingly',\n",
       " 'defeated',\n",
       " 'section',\n",
       " '4',\n",
       " 'an',\n",
       " 'experiment',\n",
       " 'demonstrating',\n",
       " 'this',\n",
       " 'counterintuitive',\n",
       " 'effect',\n",
       " 'number',\n",
       " 'papers',\n",
       " 'researchers',\n",
       " 'seemed',\n",
       " 'whether',\n",
       " 'was',\n",
       " 'lin-',\n",
       " 'guistically',\n",
       " 'salient',\n",
       " 'confidence',\n",
       " 'could',\n",
       " 're-',\n",
       " 'jected',\n",
       " 'measure',\n",
       " 'salience',\n",
       " 'whereas',\n",
       " 'they',\n",
       " 'were',\n",
       " 'merely',\n",
       " 'had',\n",
       " 'such',\n",
       " 'cases',\n",
       " 'reviewed',\n",
       " '5',\n",
       " 'widely',\n",
       " 'acquisition',\n",
       " 'subcategorization',\n",
       " 'frames',\n",
       " 'from',\n",
       " 'considered',\n",
       " 'detail',\n",
       " 'alternatives',\n",
       " 'inappropriate',\n",
       " 'hy-',\n",
       " 'pothesis-testing',\n",
       " 'presented',\n",
       " 'before',\n",
       " 'proceeding',\n",
       " 'may',\n",
       " 'i',\n",
       " 'clarify',\n",
       " 'paper',\n",
       " 'way',\n",
       " 'critical',\n",
       " 'probability',\n",
       " 'models',\n",
       " 'all',\n",
       " 'based',\n",
       " 'on',\n",
       " 'assumptions',\n",
       " 'responsible',\n",
       " 'share',\n",
       " 'progress',\n",
       " 'field',\n",
       " 'last',\n",
       " 'decade',\n",
       " 'half',\n",
       " 'untrue',\n",
       " 'preclude',\n",
       " 'them',\n",
       " 'being',\n",
       " 'useful',\n",
       " 'making',\n",
       " 'false',\n",
       " 'ingenious',\n",
       " 'proceed',\n",
       " ';',\n",
       " 'arises',\n",
       " 'literal',\n",
       " 'falsity',\n",
       " 'assumption',\n",
       " 'overlooked',\n",
       " 'inappropri-',\n",
       " 'ate',\n",
       " 'inferences',\n",
       " 'drawn',\n",
       " '2',\n",
       " 'common',\n",
       " 'parlance',\n",
       " 'synonyms',\n",
       " 'diction-',\n",
       " 'aries',\n",
       " 'giving',\n",
       " 'near-identical',\n",
       " 'definitions',\n",
       " 'ldoce',\n",
       " '1995',\n",
       " 'defines',\n",
       " 'happening',\n",
       " 'chosen',\n",
       " 'definite',\n",
       " 'plan',\n",
       " 'pattern',\n",
       " 'decided',\n",
       " 'arranged',\n",
       " 'unfairly',\n",
       " '…',\n",
       " 'by',\n",
       " 'chance',\n",
       " '265',\n",
       " 'superficially',\n",
       " 'defined',\n",
       " 'what',\n",
       " 'technical',\n",
       " 'sense',\n",
       " 'captures',\n",
       " 'makes',\n",
       " 'explicit',\n",
       " 'terms',\n",
       " 'independence',\n",
       " 'first',\n",
       " 'formalize',\n",
       " 'framework',\n",
       " 'population',\n",
       " 'events',\n",
       " 'holds',\n",
       " 'x',\n",
       " 'event',\n",
       " 'second',\n",
       " 'y',\n",
       " 'now',\n",
       " 'iff',\n",
       " 'prob-',\n",
       " 'ability',\n",
       " 'subset',\n",
       " 'hold',\n",
       " 'x|y',\n",
       " '⫽',\n",
       " 'x|ÿ',\n",
       " 'symmetric',\n",
       " 'entails',\n",
       " 'y|x',\n",
       " 'y|ÿ',\n",
       " 'hereafter',\n",
       " 'use',\n",
       " '‘',\n",
       " 'meaning',\n",
       " 'arbi-',\n",
       " 'trary',\n",
       " 'non-technical',\n",
       " 'one',\n",
       " 'rarely',\n",
       " 'takes',\n",
       " 'considerable',\n",
       " 'ingenuity',\n",
       " 'sophisticated',\n",
       " 'mathe-',\n",
       " 'matics',\n",
       " 'produce',\n",
       " 'pseudo-random',\n",
       " 'sequence',\n",
       " 'algorithmically',\n",
       " '“',\n",
       " 'defi-',\n",
       " 'nite',\n",
       " 'aim',\n",
       " '”',\n",
       " 'definition',\n",
       " 'vanish-',\n",
       " 'ingly',\n",
       " 'unlikely',\n",
       " 'outside',\n",
       " 'sub-atomic',\n",
       " 'realm',\n",
       " 'natural',\n",
       " 'consider',\n",
       " 'cat',\n",
       " 'food',\n",
       " 'purchases',\n",
       " 'shoe-polish',\n",
       " 'within',\n",
       " 'space',\n",
       " 'uk',\n",
       " 'supermarket-shopping',\n",
       " 'bought',\n",
       " 'predict',\n",
       " 'positively',\n",
       " 'negatively',\n",
       " 'shoe',\n",
       " 'polish',\n",
       " 'same',\n",
       " 'shopping',\n",
       " 'trip',\n",
       " '?',\n",
       " 'obvious',\n",
       " 'why',\n",
       " 'should',\n",
       " 'happily',\n",
       " 'declare',\n",
       " 'perhaps',\n",
       " 'either',\n",
       " 'more',\n",
       " 'less',\n",
       " 'hot',\n",
       " 'cold',\n",
       " 'weather',\n",
       " 'saturday',\n",
       " 'nights',\n",
       " 'sunday',\n",
       " 'mornings',\n",
       " 'monday',\n",
       " 'lunchtimes',\n",
       " 'richer',\n",
       " 'poorer',\n",
       " 'people',\n",
       " 'men',\n",
       " 'women',\n",
       " 'out',\n",
       " 'towns…',\n",
       " 'unlimited',\n",
       " 'hypotheses',\n",
       " 'connecting',\n",
       " 'just',\n",
       " 'these',\n",
       " 'validity',\n",
       " 'weak',\n",
       " 'point',\n",
       " 'you',\n",
       " 'question',\n",
       " 'construct',\n",
       " 'wide',\n",
       " 'tasks',\n",
       " 'although',\n",
       " 'only',\n",
       " 'strong',\n",
       " 'thus',\n",
       " '1,000',\n",
       " 'trips',\n",
       " 'un-',\n",
       " 'likely',\n",
       " 'concerning',\n",
       " 'strawberry-buying',\n",
       " 'cream-buying',\n",
       " 'further',\n",
       " '1,000,000',\n",
       " '266',\n",
       " 'also',\n",
       " 'regarding',\n",
       " 'nappy2-',\n",
       " 'buying',\n",
       " 'beer-sixpack-buying',\n",
       " 'most',\n",
       " 'newsworthy',\n",
       " 'product',\n",
       " 'large-scale',\n",
       " 'mining',\n",
       " 'supermarkets',\n",
       " 'reported',\n",
       " 'british',\n",
       " 'media',\n",
       " 'still',\n",
       " 'catf',\n",
       " 'ood',\n",
       " '1,000,000,000',\n",
       " 'eg',\n",
       " '95',\n",
       " '%',\n",
       " 'confi-',\n",
       " 'dence',\n",
       " 'function',\n",
       " 'sample',\n",
       " 'size',\n",
       " 'level',\n",
       " 'held',\n",
       " 'constant',\n",
       " 'enormous',\n",
       " 'seen',\n",
       " 'providing',\n",
       " 'support',\n",
       " 'distinguish-',\n",
       " 'role',\n",
       " 'plays',\n",
       " 'across',\n",
       " 'social',\n",
       " 'sciences',\n",
       " 'varies',\n",
       " 'order',\n",
       " 'magnitude',\n",
       " 'wrong',\n",
       " 'identify',\n",
       " 'accept-h0/reject-h0',\n",
       " 'arbitrary/motivated',\n",
       " 'uneasy',\n",
       " 'relationship',\n",
       " 'quantity',\n",
       " 'familiar',\n",
       " 'statisticians',\n",
       " 'though',\n",
       " 'mis-',\n",
       " 'understood',\n",
       " 'statistics',\n",
       " 'carver',\n",
       " '1993',\n",
       " 'stubbs',\n",
       " 'brandstätter',\n",
       " '1999',\n",
       " 'textbook',\n",
       " 'warns',\n",
       " 'none',\n",
       " 'respect',\n",
       " 'good-',\n",
       " 'ness',\n",
       " 'fit',\n",
       " 'exactly',\n",
       " 'increase',\n",
       " 'value',\n",
       " 'χ2',\n",
       " 'would',\n",
       " 'ultimately',\n",
       " 'reach',\n",
       " 'test',\n",
       " 'tell',\n",
       " 'too',\n",
       " 'small',\n",
       " '!',\n",
       " 'owen',\n",
       " 'jones',\n",
       " '1977',\n",
       " '359',\n",
       " 'particularly',\n",
       " 'firstly',\n",
       " 'access',\n",
       " 'extremely',\n",
       " 'sizes',\n",
       " 'secondly',\n",
       " 'distri-',\n",
       " 'bution',\n",
       " 'many',\n",
       " 'zipfian',\n",
       " '6,000,000',\n",
       " 'oc-',\n",
       " 'currences',\n",
       " 'bnc',\n",
       " 'spelled',\n",
       " '66',\n",
       " 'vast',\n",
       " 'third',\n",
       " 'draw',\n",
       " 'about',\n",
       " '3',\n",
       " 'objections',\n",
       " 'maximum',\n",
       " 'estimates',\n",
       " 'mles',\n",
       " 'church',\n",
       " 'hanks',\n",
       " '1990',\n",
       " 'inaugurated',\n",
       " 'research',\n",
       " 'area',\n",
       " 'lexical',\n",
       " 'statis-',\n",
       " 'tics',\n",
       " 'their',\n",
       " 'presentation',\n",
       " 'mutual',\n",
       " 'information',\n",
       " 'closely',\n",
       " 'associated',\n",
       " 'applied',\n",
       " 'finding',\n",
       " 'occur',\n",
       " 'together',\n",
       " 'noteworthy',\n",
       " 'degree',\n",
       " 'against',\n",
       " 'another',\n",
       " 'various',\n",
       " 'other',\n",
       " 'purposes.3',\n",
       " 'define',\n",
       " '267',\n",
       " 'log',\n",
       " '冉',\n",
       " 'xandy',\n",
       " '·',\n",
       " '冊',\n",
       " 'estimate',\n",
       " 'probabilities',\n",
       " 'directly',\n",
       " 'mle',\n",
       " 'f',\n",
       " '/n',\n",
       " 'x-and-y',\n",
       " 'thereby',\n",
       " 'n',\n",
       " '⫺',\n",
       " 'dunning',\n",
       " 'presents',\n",
       " 'critique',\n",
       " 'his',\n",
       " 'objection',\n",
       " 'confused',\n",
       " 'make',\n",
       " 'mention',\n",
       " 'work',\n",
       " 'while',\n",
       " 'both',\n",
       " 'valid',\n",
       " 'different',\n",
       " 'nature',\n",
       " 'independent',\n",
       " 'demonstrates',\n",
       " 'fare',\n",
       " 'poorly',\n",
       " 'estimating',\n",
       " 'rare',\n",
       " 'bigram',\n",
       " 'trigram',\n",
       " 'character-sequence',\n",
       " 'etc',\n",
       " 'occurs',\n",
       " 'once',\n",
       " 'twice',\n",
       " 'bigrams',\n",
       " 'simplest',\n",
       " 'nile',\n",
       " '1/n',\n",
       " '2/n',\n",
       " 'factor',\n",
       " 'arbitrariness',\n",
       " 'occurring',\n",
       " 'ten',\n",
       " 'times',\n",
       " 'roughly',\n",
       " 'singletons',\n",
       " 'doubletons',\n",
       " 'occurred',\n",
       " 'original',\n",
       " 'mass',\n",
       " 'contributing',\n",
       " 'put',\n",
       " 'aside',\n",
       " 'did',\n",
       " 'particular',\n",
       " 'viewed',\n",
       " 'counted',\n",
       " 'allow',\n",
       " 'occurrences',\n",
       " 'bases',\n",
       " 'assert',\n",
       " 'ways',\n",
       " 'discounting',\n",
       " 'done',\n",
       " 'good-turing',\n",
       " 'method',\n",
       " 'good',\n",
       " '1953',\n",
       " 'usefully',\n",
       " 'em-',\n",
       " 'pirical',\n",
       " 'gale',\n",
       " 'sampson',\n",
       " 'bod',\n",
       " 'advocates',\n",
       " 'log-likelihood',\n",
       " 'statistic',\n",
       " 'like',\n",
       " 'χ2-distributed,4',\n",
       " 'accurately',\n",
       " 'probabil-',\n",
       " 'ities',\n",
       " 'counts',\n",
       " 'since',\n",
       " 'pedersen',\n",
       " '1996',\n",
       " 'shown',\n",
       " 'fisher',\n",
       " 'exact',\n",
       " 'fails',\n",
       " 'represent',\n",
       " 'generally',\n",
       " 'taken',\n",
       " 'five',\n",
       " 'represented',\n",
       " 'anxieties',\n",
       " 'ease',\n",
       " '268',\n",
       " 'calculated',\n",
       " 'high',\n",
       " 'thousands',\n",
       " 'task',\n",
       " 'determine',\n",
       " 'interesting',\n",
       " 'associa-',\n",
       " 'must',\n",
       " 'heeded',\n",
       " 'high-frequency',\n",
       " 'concerns',\n",
       " 'author',\n",
       " 'discovered',\n",
       " 'grappling',\n",
       " 'following',\n",
       " 'indisputably',\n",
       " 'type',\n",
       " 'differences',\n",
       " 'each',\n",
       " 'written',\n",
       " 'part',\n",
       " 'national',\n",
       " 'sampling',\n",
       " 'follows',\n",
       " 'texts',\n",
       " 'shorter',\n",
       " '20,000',\n",
       " 'excluded',\n",
       " 'left',\n",
       " '820',\n",
       " 'assigned',\n",
       " 'subcorpora',\n",
       " 'col-',\n",
       " 'lections',\n",
       " 'samples',\n",
       " 'consequently',\n",
       " 'deviation',\n",
       " 'frequency',\n",
       " 'occurrence',\n",
       " 'individual',\n",
       " 'sub-',\n",
       " 'explicable',\n",
       " 'fluctuation',\n",
       " 'ho',\n",
       " 'tested',\n",
       " 'χ2-test',\n",
       " 'σ',\n",
       " '|o',\n",
       " 'e',\n",
       " '|',\n",
       " '0.5',\n",
       " '/e',\n",
       " 'greater',\n",
       " 'sum',\n",
       " 'over',\n",
       " 'four',\n",
       " 'cells',\n",
       " 'contingency',\n",
       " 'table',\n",
       " 'w',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'assign',\n",
       " 'opposed',\n",
       " 'straightforward',\n",
       " 'distribution',\n",
       " 'χ2-statistic',\n",
       " 'equal',\n",
       " '99.5',\n",
       " 'threshold',\n",
       " '7.88',\n",
       " 'average',\n",
       " 'error',\n",
       " 'term',\n",
       " '269',\n",
       " 'e|',\n",
       " '0.5.5',\n",
       " 'therefore',\n",
       " 'couched',\n",
       " 'wary',\n",
       " 'attributing',\n",
       " 'significant',\n",
       " 'text',\n",
       " 'types',\n",
       " 'obtain',\n",
       " 'lists',\n",
       " 'word-pos',\n",
       " 'subcorpus',\n",
       " 'gener-',\n",
       " 'ated',\n",
       " 'contributed',\n",
       " 'calculation',\n",
       " 'determined',\n",
       " 'shows',\n",
       " 'values',\n",
       " 'far',\n",
       " 'tend',\n",
       " 'increases',\n",
       " 'averages',\n",
       " 'indicate',\n",
       " '*',\n",
       " '3.94',\n",
       " 'relevant',\n",
       " 'chi-square',\n",
       " 'including',\n",
       " 'hypthesis',\n",
       " 'subc-',\n",
       " 'orpora',\n",
       " 'wholes',\n",
       " 'priori',\n",
       " 'expect',\n",
       " 'behave',\n",
       " 'selected',\n",
       " 'comparing',\n",
       " 'same-genre',\n",
       " 'class',\n",
       " 'item',\n",
       " 'mean',\n",
       " 'freq',\n",
       " 'items',\n",
       " 'pos',\n",
       " '10',\n",
       " 'det',\n",
       " '18.76',\n",
       " 'next',\n",
       " 'prp',\n",
       " '17.45',\n",
       " '20',\n",
       " 'xx',\n",
       " '14.39',\n",
       " '40',\n",
       " 'vhb',\n",
       " '10.71',\n",
       " '80',\n",
       " 'avo',\n",
       " '7.03',\n",
       " '160',\n",
       " 'know',\n",
       " 'vvi',\n",
       " '6.40',\n",
       " '320',\n",
       " 'six',\n",
       " 'crd',\n",
       " '5.30',\n",
       " '640',\n",
       " 'finally',\n",
       " 'av0',\n",
       " '6.71',\n",
       " '1280',\n",
       " 'plants',\n",
       " 'nn2',\n",
       " '6.05',\n",
       " '2560',\n",
       " 'pocket',\n",
       " 'nn1',\n",
       " '5.82',\n",
       " '5120',\n",
       " 'vvb',\n",
       " '4.53',\n",
       " '10240',\n",
       " 'peking',\n",
       " 'np0',\n",
       " '3.07',\n",
       " '20480',\n",
       " 'fondly',\n",
       " '1.87',\n",
       " '40960',\n",
       " 'chandelier',\n",
       " '1.15',\n",
       " 'note',\n",
       " 'tags',\n",
       " 'claws-5',\n",
       " 'tagset',\n",
       " 'see',\n",
       " 'http',\n",
       " '/info.ox',\n",
       " 'ac.uk/bnc',\n",
       " '270',\n",
       " 'collections',\n",
       " 'covering',\n",
       " 'registers',\n",
       " 'comprising',\n",
       " 'say',\n",
       " 'thousand',\n",
       " 'seem',\n",
       " 'plausible',\n",
       " 'oddities',\n",
       " 'balance',\n",
       " 'give',\n",
       " 'popula-',\n",
       " 'indistinguishable',\n",
       " 'turns',\n",
       " 'case',\n",
       " 'key',\n",
       " 'paragraph',\n",
       " 'objective',\n",
       " 'distin-',\n",
       " 'guished',\n",
       " 'generated',\n",
       " 'populations',\n",
       " 'distinguishable',\n",
       " 'basis',\n",
       " 'joint',\n",
       " 'firms',\n",
       " 're-analysis',\n",
       " 'previous',\n",
       " '5.1',\n",
       " 'brown',\n",
       " 'lob',\n",
       " 'hofland',\n",
       " 'johansson',\n",
       " '1982',\n",
       " 'wanted',\n",
       " 'find',\n",
       " 'signifi-',\n",
       " 'cantly',\n",
       " 'american',\n",
       " 'eng-',\n",
       " 'lish',\n",
       " 'english',\n",
       " 'difference',\n",
       " 'explained',\n",
       " 'variation',\n",
       " 'source',\n",
       " 'mark',\n",
       " '99',\n",
       " '99.9',\n",
       " 'looking',\n",
       " 'suggests',\n",
       " 'virtually',\n",
       " 'markedly',\n",
       " 'levels',\n",
       " 'marked',\n",
       " 'contrast',\n",
       " 'rarer',\n",
       " 'refer',\n",
       " 'argument',\n",
       " 'explains',\n",
       " 'simply',\n",
       " 'consequence',\n",
       " 'essen-',\n",
       " 'tially',\n",
       " 'surprising',\n",
       " 'repeat',\n",
       " 'new',\n",
       " 'similar',\n",
       " 'strategies',\n",
       " 'applicable',\n",
       " 'leech',\n",
       " 'fallon',\n",
       " '1992',\n",
       " 'again',\n",
       " 'ray-',\n",
       " '271',\n",
       " 'son',\n",
       " 'hodges',\n",
       " '1997',\n",
       " 'conversation',\n",
       " 'dif-',\n",
       " 'ferent',\n",
       " 'groups',\n",
       " 'rayson',\n",
       " 'garside',\n",
       " '2000',\n",
       " 'contrasting',\n",
       " 'specialist',\n",
       " 'genre',\n",
       " '5.2',\n",
       " 'frame',\n",
       " 'scf',\n",
       " 'learning',\n",
       " 'automatic',\n",
       " 'scfs',\n",
       " 'verbs',\n",
       " 'dictionaries',\n",
       " 'accurate',\n",
       " 'complete',\n",
       " 'sent',\n",
       " 'rise',\n",
       " 'parsing',\n",
       " 'errors',\n",
       " 'brent',\n",
       " 'briscoe',\n",
       " 'carroll',\n",
       " 'kor-',\n",
       " 'honen',\n",
       " ...]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련시키면 이곳에 append 됨\n",
    "list(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cd191c4-da19-4b10-86ac-04618a4ef888",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2b9a9d9-2fd6-4e69-8917-9fceb481e8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1391"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.vocab)\n",
    "\n",
    "# 중복 제거된 1391개\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2e97eed-51d6-4b6c-a835-90fb160911a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.score()\n",
    "# language 다음에 is가 올 확률 체크\n",
    "model.score('is', ['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e1f9d57-f2a1-4294-9079-3f81570e016d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.5 이상이라 추천할만한 Data로 적합\n",
    "model.score('never', ['language', 'is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8476876-c88e-4d81-a400-d375e5e53db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = list(model.vocab)\n",
    "\n",
    "recommends = []\n",
    "\n",
    "for word in vocabs :\n",
    "\n",
    "    score = model.score(word, ['language', 'is'])\n",
    "\n",
    "    if score >= 0.5 :\n",
    "\n",
    "        recommends.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbe223-1a51-43ea-90f0-2c06a31b16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language is 다음에 올 확률이 0.5 이상인 Toekn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0db4b272-1ac5-47b2-a8a8-0f24f4f9bbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cfbc69d-dcc0-4bed-9880-6946450f0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moodel.generate(Token 개수)\n",
    "# 개수만큼 문장을 랜덤하게 학습한 구조에 맞게 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c66298cc-16a4-4b27-a92f-fcd1e89ed288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brill',\n",
       " '(',\n",
       " '2001',\n",
       " ')',\n",
       " 'for',\n",
       " 'verbs',\n",
       " 'from',\n",
       " 'the',\n",
       " 'anlt',\n",
       " 'lexicon',\n",
       " ':',\n",
       " 'see',\n",
       " 'briscoe',\n",
       " 'and',\n",
       " 'carroll',\n",
       " '’',\n",
       " 's',\n",
       " 'concern',\n",
       " 'must',\n",
       " 'be']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025de26-4811-47ab-8540-529cb00ffd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 형태의 Data를 문장으로 변환해주는 편의 기능\n",
    "# Token -> 문장 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "570a67c4-c701-44ef-9869-7a13abcf60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52478fef-0ea2-4352-b5a9-17df049b57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = dt().detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e37e01b3-d952-4e77-babf-d8ffc3cf9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, token_count) :\n",
    "\n",
    "    sentences = model.generate(token_count)\n",
    "\n",
    "    content = []\n",
    "\n",
    "    for word in sentences :\n",
    "\n",
    "        if word == '<s>' :\n",
    "\n",
    "            continue\n",
    "            \n",
    "        if word == '</s>' :\n",
    "\n",
    "            break\n",
    "\n",
    "        content.append(word)\n",
    "\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fcc91175-fc4f-4fc6-ba58-463fd3f2d6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cells of the joint conference on applied natural language processing.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 자동 생성\n",
    "generate_sentence(model, 12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
